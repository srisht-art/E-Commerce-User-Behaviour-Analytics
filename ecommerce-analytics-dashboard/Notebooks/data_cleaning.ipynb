{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning Script\n",
        "\n",
        "**Dataset:** `./ecommerce_user_segmentation.csv`\n",
        "\n",
        "## Steps:\n",
        "1. Load & inspect\n",
        "2. Clean (missing values, dates, strings, columns)\n",
        "3. Handle duplicates\n",
        "4. Detect outliers (IQR) â€“ DO NOT REMOVE\n",
        "5. Final checks & summary\n",
        "6. Export cleaned dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. IMPORT LIBRARIES & LOAD DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1.1 Load the dataset\n",
        "file_path = \"../Data/ecommerce_user_segmentation.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 1.2 Display first 10 rows\n",
        "print(\"\\n===== First 10 rows of the dataset =====\")\n",
        "print(df.head(10))\n",
        "\n",
        "# 1.3 Display shape (rows, columns)\n",
        "print(\"\\n===== Dataset Shape (rows, columns) =====\")\n",
        "print(df.shape)\n",
        "\n",
        "# 1.4 Display data types per column\n",
        "print(\"\\n===== Column Data Types =====\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# 1.5 Display missing values count per column\n",
        "print(\"\\n===== Missing Values per Column =====\")\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CLEAN THE DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Fill missing values in NUMERICAL columns with MEAN\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "print(\"\\n===== Numerical Columns (for mean imputation) =====\")\n",
        "print(list(numeric_cols))\n",
        "\n",
        "for col in numeric_cols:\n",
        "    mean_value = df[col].mean()\n",
        "    df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# 2.2 Fill missing values in CATEGORICAL columns with MODE\n",
        "# Include 'object', 'category', and pandas \"string\" dtype as categorical\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category', 'string']).columns\n",
        "print(\"\\n===== Categorical Columns (for mode imputation) =====\")\n",
        "print(list(categorical_cols))\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        mode_value = df[col].mode(dropna=True)\n",
        "        if not mode_value.empty:\n",
        "            df[col].fillna(mode_value.iloc[0], inplace=True)\n",
        "\n",
        "# 2.3 Convert date/time columns (if any) into datetime format\n",
        "# Here we infer date/time columns based on column names containing common keywords\n",
        "date_like_keywords = [\"date\", \"time\", \"datetime\", \"timestamp\"]\n",
        "date_cols = [col for col in df.columns\n",
        "             if any(kw in col.lower() for kw in date_like_keywords)]\n",
        "\n",
        "print(\"\\n===== Columns considered as Date/Time =====\")\n",
        "print(date_cols)\n",
        "\n",
        "for col in date_cols:\n",
        "    # errors='coerce' will turn invalid dates into NaT\n",
        "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# 2.4 Strip whitespace from all string/text (object) columns\n",
        "for col in categorical_cols:\n",
        "    # Only apply strip if the column is of object/string type\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# 2.5 Convert all column names to lowercase and replace spaces with underscores\n",
        "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "print(\"\\n===== Column Names After Standardization =====\")\n",
        "print(list(df.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. HANDLE DUPLICATE ENTRIES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Count duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(\"\\n===== Duplicate Rows Count =====\")\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "# 3.2 Remove duplicates\n",
        "before_rows = df.shape[0]\n",
        "df = df.drop_duplicates()\n",
        "after_rows = df.shape[0]\n",
        "\n",
        "# 3.3 Print how many rows were removed\n",
        "removed_rows = before_rows - after_rows\n",
        "print(\"\\n===== Duplicate Removal Summary =====\")\n",
        "print(f\"Rows before removing duplicates: {before_rows}\")\n",
        "print(f\"Rows after removing duplicates:  {after_rows}\")\n",
        "print(f\"Total duplicate rows removed:    {removed_rows}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. DETECT OUTLIERS USING IQR (DO NOT REMOVE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n===== Outlier Detection using IQR (per numerical column) =====\")\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns  # re-evaluate after cleaning\n",
        "outlier_summary = {}\n",
        "\n",
        "for col in numeric_cols:\n",
        "    col_data = df[col].dropna()\n",
        "    if col_data.empty:\n",
        "        outlier_summary[col] = 0\n",
        "        continue\n",
        "\n",
        "    Q1 = col_data.quantile(0.25)\n",
        "    Q3 = col_data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 + (1.5 * -IQR)  # same as Q1 - 1.5*IQR\n",
        "    upper_bound = Q3 + (1.5 * IQR)\n",
        "\n",
        "    outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
        "    outlier_count = outliers.shape[0]\n",
        "    outlier_summary[col] = outlier_count\n",
        "\n",
        "    print(f\"Column '{col}': {outlier_count} outliers (IQR method)\")\n",
        "\n",
        "# Short interpretation for outliers\n",
        "print(\"\\n===== Outlier Interpretation (High-level) =====\")\n",
        "for col, count in outlier_summary.items():\n",
        "    if count > 0:\n",
        "        print(f\"- '{col}' has {count} potential outliers. \"\n",
        "              f\"Business team may decide later whether these are real extreme cases \"\n",
        "              f\"(e.g., very high spenders) or data errors.\")\n",
        "    else:\n",
        "        print(f\"- '{col}' has no strong outliers detected by IQR.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. FINAL CHECKS AFTER CLEANING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1 Print missing values again to confirm cleanup\n",
        "print(\"\\n===== Missing Values per Column AFTER Cleaning =====\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 5.2 Print final dataset shape\n",
        "print(\"\\n===== Final Dataset Shape (rows, columns) =====\")\n",
        "print(df.shape)\n",
        "\n",
        "# 5.3 Beginner-friendly summary of what was cleaned and why\n",
        "print(\"\\n===== Cleaning Summary (Beginner-Friendly) =====\")\n",
        "summary_points = [\n",
        "    \"1. Filled empty values in number columns with the average (mean) so that calculations work smoothly.\",\n",
        "    \"2. Filled empty values in text/category columns with the most common value (mode) to keep categories consistent.\",\n",
        "    \"3. Converted any date/time-like columns into proper datetime format so we can do time-based analysis later.\",\n",
        "    \"4. Removed extra spaces from text fields to avoid treating 'User ' and 'User' as different values.\",\n",
        "    \"5. Standardized column names to lowercase_with_underscores, making them easier to use in code.\",\n",
        "    \"6. Detected and removed duplicate rows so each user/record is counted only once.\",\n",
        "    \"7. Identified potential outliers in numeric columns using the IQR method, but did NOT remove them yet.\",\n",
        "    \"8. Verified that there are no remaining missing values and checked the final size of the cleaned dataset.\"\n",
        "]\n",
        "\n",
        "for point in summary_points:\n",
        "    print(point)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. EXPORT THE CLEANED DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = \"../Data/cleaned_ecommerce_dataset.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\n===== Export Complete =====\")\n",
        "print(f\"Cleaned dataset saved as: {output_file} (index=False)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
